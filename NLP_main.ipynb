{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing InShorts articles using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval with Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n",
      "sports\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "start_urls = [\n",
    "    'https://inshorts.com/en/read/technology',\n",
    "    'https://inshorts.com/en/read/sports',\n",
    "    'https://inshorts.com/en/read/world'\n",
    "]\n",
    "\n",
    "def create_dataset(start_urls):\n",
    "    news_data = []\n",
    "    for url in start_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        print(news_category)\n",
    "create_dataset(start_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(start_urls):\n",
    "    news_data = []\n",
    "    for url in start_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        \n",
    "        news_articles = [{\n",
    "            'news_headline': headline.find('span',attrs = {\"itemprop\":\"headline\"}).text,\n",
    "            'news_article': article.find('div',attrs = {\"itemprop\":\"articleBody\"}).text,\n",
    "            'news_category':news_category}\n",
    "            \n",
    "            for headline, article in\n",
    "                             zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', \n",
    "                                               class_=[\"news-card-content news-right-box\"]))\n",
    "         ]\n",
    "        news_data.extend(news_articles)\n",
    "    df = pd.DataFrame(news_data)\n",
    "    df = df[['news_headline', 'news_article', 'news_category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       news_headline  \\\n",
      "0  Katy Perry performs at first-ever OnePlus Musi...   \n",
      "1  I'm a weird case, obviously don't need to work...   \n",
      "2  Apple announces special event for apps and gam...   \n",
      "3  Google's room-sized 331 LED bulb system create...   \n",
      "4  Realme CEO tweets update about company's phone...   \n",
      "\n",
      "                                        news_article news_category  \n",
      "0  The OnePlus Music Festival, held at Mumbai's D...    technology  \n",
      "1  Microsoft Co-founder Bill Gates, at a recent e...    technology  \n",
      "2  Apple is hosting a special media event that is...    technology  \n",
      "3  Google AI team has built a room-sized system c...    technology  \n",
      "4  Realme India CEO Madhav Sheth recently tweeted...    technology  \n"
     ]
    }
   ],
   "source": [
    "data_df = create_dataset(start_urls) \n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports        25\n",
       "technology    25\n",
       "world         25\n",
       "Name: news_category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.news_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Wrangling & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Stop words from nltk\n",
    "\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "import spacy # For importing English Language Model\n",
    "\n",
    "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' HeLLo World '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text,'html.parser')\n",
    "    filtered_text = soup.get_text()\n",
    "    return filtered_text\n",
    "\n",
    "remove_html('<h2> HeLLo World </h2><br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some text'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accent(text):\n",
    "    text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return text\n",
    "remove_accent('Sómě těxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
